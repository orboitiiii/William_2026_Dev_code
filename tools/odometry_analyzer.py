#!/usr/bin/env python3
"""
Advanced Odometry GNC Optimizer (Aerospace-grade)

Architecture:
1. Feature Selection (Random Forest): Identify top 3-5 critical features.
2. Physics Modeling (Symbolic): Stribeck, Voltage Sag, Linear models.
3. Trajectory Optimization (Simulation-in-the-loop):
   - Vectorized Dead Reckoning Integration (NumPy)
   - Cost Function: Final Pose Error (Euclidean)
   - Optimizer: L-BFGS-B / Powell

Author: Auto-generated by Antigravity
"""

import argparse
import time
from typing import Callable, Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy.optimize import minimize
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler

# === Constants ===
WHEEL_RADIUS = 0.0508  # ~2 inches in meters
TRACK_WIDTH = 0.527  # ~20.75 inches
WHEEL_BASE = 0.527


class FeatureSelector:
    """
    Selects the 'Critical Few' features using Random Forest Importance.
    Filters noise to prevent curse of dimensionality.
    """

    def __init__(self, top_k: int = 4):
        self.top_k = top_k
        self.selected_features: List[str] = []
        self.scaler = StandardScaler()

    def select(
        self,
        summary_df: pd.DataFrame,
        feature_cols: List[str],
        target_col: str = "error_trans",
    ) -> List[str]:
        """
        Train RF on trial summaries to find features that predict final error.
        """
        X = summary_df[feature_cols].fillna(0).values
        y = summary_df[target_col].values

        # Scale features
        X_scaled = self.scaler.fit_transform(X)

        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X_scaled, y)

        importances = pd.DataFrame(
            {"feature": feature_cols, "importance": rf.feature_importances_}
        ).sort_values("importance", ascending=False)

        print("\n=== Feature Importance Analysis (The Filter) ===")
        print(importances.head(10).to_string(index=False))

        self.selected_features = importances["feature"].head(self.top_k).tolist()
        print(f"\nLocked Parameters (Top {self.top_k}): {self.selected_features}")

        return self.selected_features


class PhysicsModel:
    """Base class for Odometry Correction Models."""

    def __init__(self, name: str, feature_names: List[str]):
        self.name = name
        self.feature_names = feature_names

    def get_initial_guess(self) -> np.ndarray:
        raise NotImplementedError

    def correct_velocity(
        self, v_raw: np.ndarray, features: np.ndarray, params: np.ndarray
    ) -> np.ndarray:
        """
        Apply correction to raw wheel velocities.
        v_raw: (N, 4) array of wheel velocities
        features: (N, K) array of selected features
        params: Optimization parameters
        """
        raise NotImplementedError


class LinearModel(PhysicsModel):
    """
    Model A: Linear Correction
    v_corr = v_raw * (1 + sum(k_i * f_i))
    """

    def get_initial_guess(self) -> np.ndarray:
        return np.zeros(len(self.feature_names))  # Start with 0 correction

    def correct_velocity(
        self, v_raw: np.ndarray, features: np.ndarray, params: np.ndarray
    ) -> np.ndarray:
        # correction_factor = 1 + k1*f1 + k2*f2 ...
        # params: [k1, k2, ...]

        # (N, K) @ (K,) -> (N,)
        correction_term = features @ params

        # Broadcast to (N, 4)
        factor = 1.0 + correction_term[:, np.newaxis]
        return v_raw * factor


class StribeckModel(PhysicsModel):
    """
    Model B: Stribeck Friction + Linear Features
    v_corr = v_raw * (1 + k_st * exp(-|v|/v_st) + sum(k_i * f_i))
    params: [v_st, k_st, k1, k2...]
    """

    def get_initial_guess(self) -> np.ndarray:
        # Guess: v_st=0.1m/s, k_st=0.01, others=0
        return np.concatenate(([0.1, 0.01], np.zeros(len(self.feature_names))))

    def correct_velocity(
        self, v_raw: np.ndarray, features: np.ndarray, params: np.ndarray
    ) -> np.ndarray:
        v_st = np.abs(params[0]) + 1e-6  # Ensure positive non-zero
        k_st = params[1]
        linear_params = params[2:]

        # Stribeck term: depends on velocity magnitude
        # v_raw is (N, 4)
        term_stribeck = k_st * np.exp(-np.abs(v_raw) / v_st)

        # Linear term: depends on external features
        term_linear = (features @ linear_params)[:, np.newaxis]

        factor = 1.0 + term_stribeck + term_linear
        return v_raw * factor


class VoltageSagModel(PhysicsModel):
    """
    Model C: Voltage Sag
    v_corr = v_raw * (1 + k_v / V_bat + ...)
    Requires 'battery_voltage' to be in features or handled separately.
    For simplicity, we assume generic features include voltage if selected.
    """

    # ... (Implemented as LinearModel variant usually, unless specific V_bat logic needed)
    pass


class TrajectoryOptimizer:
    """
    Core GNC Engine: Optimization-via-Integration.
    Uses vectorized NumPy operations to replay trajectories 100x faster than loops.
    """

    def __init__(
        self, timeseries: pd.DataFrame, trials: pd.DataFrame, feature_cols: List[str]
    ):
        self.ts_data = timeseries.sort_values(["trial_id", "timestamp"])
        self.trials = trials
        self.feature_cols = feature_cols

        # Pre-compute and cache NumPy arrays for speed
        self._cache_data()

    def _cache_data(self):
        """Convert DataFrame columns to structured NumPy arrays for vectorization."""
        print("Caching data for vectorized replay...")

        # Group by trial_id to get start/end indices
        self.trial_indices = {}
        grouped = self.ts_data.groupby("trial_id")

        # We need continuous arrays, but split by trial
        # Map trial_id -> (start_idx, end_idx)
        self.trial_ids = sorted(self.ts_data["trial_id"].unique())

        # Extract columns
        self.timestamps = self.ts_data["timestamp"].values
        self.dt = self.ts_data["dt_actual"].values
        self.dt[np.isnan(self.dt)] = 0.02

        # Raw Wheel Velocities (N, 4)
        self.v_wheels = self.ts_data[[f"wheel_drive_vel_{i}" for i in range(4)]].values
        self.theta_steers = self.ts_data[
            [f"wheel_steer_angle_{i}" for i in range(4)]
        ].values
        self.gyro_omega = self.ts_data["omega"].values

        # Features (N, K)
        self.features_arr = self.ts_data[self.feature_cols].fillna(0).values

        # Normalize features globally
        self.scaler = StandardScaler()
        self.features_arr = self.scaler.fit_transform(self.features_arr)

        # Pre-calculate slice indices for fast splitting
        # This allows us to run cost function per trial or batched
        # For 'minimize', we usually sum error over all trials

        # Compute trial start/counts
        counts = grouped.size()
        self.trial_slices = []
        start = 0
        for tid in self.trial_ids:
            c = counts[tid]
            self.trial_slices.append((start, start + c, tid))
            start += c

    def vectorized_replay(
        self, model: PhysicsModel, params: np.ndarray
    ) -> Tuple[float, float, float]:
        """
        Replay ALL trials using vectorized operations.
        Returns: Total SSE (Sum Squared Error)
        """
        # 1. Apply correction model (Vectorized over all N points)
        # v_wheels_corr: (N, 4)
        v_corr = model.correct_velocity(self.v_wheels, self.features_arr, params)

        # 2. Kinematics (Swerve to Chassis) - Rigorous Inverse Kinematics
        # Pauli's Critique: Simple mean is invalid during rotation.
        # v_wheel = v_robot + omega x r
        # v_robot = v_wheel - omega x r
        # v_rx = v_wx + omega * r_y
        # v_ry = v_wy - omega * r_x

        # Module positions (Standard FRC ordering: FL, FR, BL, BR)
        # x: [+, +, -, -], y: [+, -, +, -]
        L, W = WHEEL_BASE / 2.0, TRACK_WIDTH / 2.0
        r_x = np.array([L, L, -L, -L])
        r_y = np.array([W, -W, W, -W])

        # Calculate wheel velocity vectors in robot frame
        vx_wheels = v_corr * np.cos(self.theta_steers)
        vy_wheels = v_corr * np.sin(self.theta_steers)

        # Remove rotational component (tangential velocity)
        # Broadcast omega: (N,) -> (N, 4)
        omega_broad = self.gyro_omega[:, np.newaxis]

        # Apply inverse kinematic transform
        vx_robot_est = vx_wheels + omega_broad * r_y
        vy_robot_est = vy_wheels - omega_broad * r_x

        # Now we can safely average to get chassis translation
        vx_robot = np.mean(vx_robot_est, axis=1)
        vy_robot = np.mean(vy_robot_est, axis=1)

        # 3. Integration (World Frame)
        # We need to integrate orientation first.
        # Omega is trusted from Gyro (unless we are correcting gyro too).
        # Assuming Gyro is ground truth for heading in this context or simplified.

        # Since we have disconnected trials, we cannot just cumsum the whole array.
        # We must integrate strictly within trial boundaries.
        # However, looping 100 trials in Python is slow.
        # Trick: Use a segment_cumsum or masking.
        # Or simpler: Isolate trial processing?
        # Actually, Python "for trial" loop on 50 trials is negligible (50 iters).
        # "for point" loop (50 * 500 = 25000 iters) is the killer.

        total_error_sq = 0.0

        for start, end, tid in self.trial_slices:
            # Slicing numpy arrays is O(1) view
            dt_slice = self.dt[start:end]
            vx_r = vx_robot[start:end]
            vy_r = vy_robot[start:end]
            omega_slice = self.gyro_omega[start:end]

            # Heading Integration
            # theta(t) = theta(0) + cumsum(omega * dt)
            # theta(0) assumed 0 for relative odometry trial
            d_theta = omega_slice * dt_slice
            theta = np.cumsum(d_theta)
            # Use Midpoint or standard Euler? Euler is: theta[i] used for v[i]

            # Field Transformation
            # vx_field = vx_r * cos(theta) - vy_r * sin(theta)
            # vy_field = vx_r * sin(theta) + vy_r * cos(theta)
            # Precompute cos/sin
            c_th = np.cos(theta)
            s_th = np.sin(theta)

            vx_field = vx_r * c_th - vy_r * s_th
            vy_field = vx_r * s_th + vy_r * c_th

            # Position Integration
            # x = cumsum(vx_field * dt)
            x_traj = np.cumsum(vx_field * dt_slice)
            y_traj = np.cumsum(vy_field * dt_slice)

            # Final Pose Error
            final_x = x_traj[-1]
            final_y = y_traj[-1]

            # Target is (0,0) (Return to Origin trial)
            # Error = x^2 + y^2
            # Note: If trials didn't end at (0,0), we would compare against Ground Truth.
            # Assuming 'OdometryError' recorded in Summary is the TRUTH of where we ended up?
            # NO. The user log has 'poseX', 'poseY' which is ODOMETRY.
            # The 'OdometryError' struct (dx, dy) is (FinalPose - Origin).
            # We want to MINIMIZE the difference between SimulatedPose and TruePose?
            # Wait. In a "Return to Origin" test:
            # 1. Robot drives.
            # 2. Driver stops at what they think is (0,0) physically (or marked spot).
            # 3. We record 'OdometryError' = OdometryPose - PhysicalOrigin(0,0).
            #    If Odometry was perfect, it would read (0,0).
            #    If Odometry reads (0.1, 0.1), that IS the error.
            #
            # CORRECTION GOAL:
            # We want SimulatedPose derived from corrected inputs to match PHYSICAL Reality.
            # Physical Reality at end = (0,0).
            # So we want SimulatedPose_End -> (0,0).

            err_sq = final_x**2 + final_y**2
            total_error_sq += err_sq

        return total_error_sq

    def optimize(self, model: PhysicsModel):
        """Run Scipy Optimization."""
        x0 = model.get_initial_guess()
        print(f"Starting optimization for {model.name}...")
        print(f"Initial params: {x0}")

        start_t = time.time()

        def func(p):
            return self.vectorized_replay(model, p)

        res = minimize(
            func, x0, method="L-BFGS-B", options={"disp": True, "maxmult": 20}
        )

        dt = time.time() - start_t
        print(f"Optimization finished in {dt:.2f}s")
        print(f"Success: {res.success}, Final Cost: {res.fun:.4f}")
        print(f"Optimal Params: {res.x}")

        return res


def main():
    parser = argparse.ArgumentParser(description="Advanced Odometry GNC Optimizer")
    parser.add_argument("--summary", required=True)
    parser.add_argument("--timeseries", required=True)
    parser.add_argument("--output", default=".")
    args = parser.parse_args()

    print("Loading data...")
    summary = pd.read_csv(args.summary)
    ts = pd.read_csv(args.timeseries)

    # 1. Feature Selection
    # Aggregated features for selection
    # We need to map time-series features to summary for RF
    # Simpler: Use pre-computed columns in summary or compute means
    # Assuming 'OdometryTrial' logic put summary stats in summary csv from Java?
    # Or we construct them.
    # For now, let's assume summary CSV has columns like 'wheelSpeedVariance' if we logged them.
    # If not, we might need to aggregate TS.
    # Let's trust FeatureSelector on what's available.

    # Mocking available potential features for selection (from OdometryDataPoint columns)
    potential_features = [
        c
        for c in ts.columns
        if c not in ["timestamp", "dt_actual", "pose_x", "pose_y", "heading"]
    ]
    # Filter to numeric
    potential_features = [
        c
        for c in potential_features
        if "wheel" in c or "voltage" in c or "current" in c
    ]

    # We need to aggregate TS to Summary for RF selection phases
    # "The Filter" needs to run on (Trial_Stat -> Trial_Error)
    print("Aggregating time-series for feature selection...")
    agg_df = ts.groupby("trial_id")[potential_features].mean().reset_index()
    # Merge with target error
    summary_merged = pd.merge(summary, agg_df, on="trial_id")

    selector = FeatureSelector(top_k=4)
    # Target: 'error_trans' (total translation error)
    top_features = selector.select(summary_merged, potential_features, "error_trans")

    # 2. Optimization
    optimizer = TrajectoryOptimizer(ts, summary, top_features)

    models = [
        LinearModel("Linear", top_features),
        StribeckModel("Stribeck", top_features),
    ]

    best_res = None
    best_model = None
    min_cost = float("inf")

    for m in models:
        print(f"\n--- Optimizing {m.name} Model ---")
        res = optimizer.optimize(m)
        if res.fun < min_cost:
            min_cost = res.fun
            best_res = res
            best_model = m

    # 3. Output
    if best_model:
        print(f"\nWINNER: {best_model.name} (Cost: {min_cost:.4f})")

        # Export Java
        coeffs = best_res.x
        feature_names = best_model.feature_names

        # NOTE: If Stribeck, param struct is [v_st, k_st, linear...]
        # We need to format this for Java carefully.

        java_content = generate_java_class(best_model, coeffs, optimizer.scaler)

        out_path = f"{args.output}/OdometryCorrection.java"
        with open(out_path, "w") as f:
            f.write(java_content)
        print(f"Generated: {out_path}")


def generate_java_class(
    model: PhysicsModel, params: np.ndarray, scaler: StandardScaler
) -> str:
    # Logic to map model type + params to Java code
    # Simplified string generation

    feature_arr = "{" + ", ".join([f'"{f}"' for f in model.feature_names]) + "}"
    mean_arr = "{" + ", ".join([f"{x:.6f}" for x in scaler.mean_]) + "}"
    scale_arr = "{" + ", ".join([f"{x:.6f}" for x in scaler.scale_]) + "}"

    # Params
    params_arr = "{" + ", ".join([f"{x:.8f}" for x in params]) + "}"

    model_type = "LINEAR"
    if isinstance(model, StribeckModel):
        model_type = "STRIBECK"

    return f"""
/** 
 * Auto-Generated Odometry Correction 
 * Model: {model.name}
 */
public class OdometryCorrection {{
    public static final String MODEL_TYPE = "{model_type}";
    public static final String[] FEATURES = {feature_arr};
    public static final double[] MEANS = {mean_arr};
    public static final double[] SCALES = {scale_arr};
    public static final double[] PARAMS = {params_arr};
    
    // Implementation in Drive.java should switch on MODEL_TYPE
}}
"""


if __name__ == "__main__":
    main()
